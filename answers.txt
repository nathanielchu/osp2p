Questions 1.1
1. 100 threads, 500 iterations; or 2 threads, 5000 iterations fail pretty consistently. We need to have several threads and many iterations to have enough of a chance of a thread being interrupted at the right moment in the middle of its summation, which would expose the race condition.

2. With few iterations, threads are likely to finish executing before a clock interrupt. In other words, the conditions for the race condition are unlikely to occur.

Note: see timing1.1_time_iter.txt and plot1.1_time_iter.pdf for the timing and graph of time per operation against number of iterations. These timings were measured with --yield disabled.


Questions 1.2
1. With increasing iterations, the overhead time of the parent program contributes less to the total elapsed time, so the time per operation more accurately reflects the time needed just for the addition, which is relatively fast.

2. The "correct" cost is the time per operation as the number of iterations approaches infinity, which would make the overhead time negligible. In this case, it appears to be around 8 nanoseconds.

3. --yield takes more time since the program traps and possibly context switches in every single iteration, adding a large amount of overhead.

4. Valid timings cannot really be obtained with --yield, since the overhead of trapping/context switching happens with every iteration (i.e. it scales with the number of iterations). The overhead of the parent process, on the other hand, is constant, and can be reduced by increasing the number of iterations.


Questions 1.3
1. All of the options perform similarly for low numbers of threads because for low numbers of threads the situation where a thread attempts to get a lock and does not succeed becomes more likely. The main difference between these options is their implementation in addressing this situation.

2. The three protected operations slow down as the number of threads rises because as the number of threads rises the situation where a thread attempts to get a lock and does not succeed increases.

3. Spin-locks are so expensive for large numbers of threads because when a thread tries to lock a spin-lock and it does not succeed, it will continuously re-try locking it, until it finally succeeds. As the number of threads increases, the situation where a thread attempts to lock a spin-lock and does not succeed increases. Thus, for large numbers of threads spin-locks are very expensive because the polling will constantly waste a lot of CPU time.

Note: see timing1.3_time_threads.txt and plot1.3_time_threads.pdf for the timing and graph of time per operation against number of threads. These timings were measured with --yield enabled.


================================


Questions 2.1
1. As in part 1, the time per operation decreases (as an inverse relationship) with the number of iterations, approaching a limit as the number of iterations grows sufficiently high. Due to the many more operations that are performed on the list (which scales with the square of the number of iterations, unlike part 1), the time per operation drops much faster than it did with adding. Again, with fewer iterations, overhead from creating the threads likely contributes to the inflated time per operation measurement; with many iterations, this overhead (which does not scale with the number of iterations) is more negligible compared to the time spent actually manipulating the list.

Note: see timing2.1_time_iter.txt and plot2.1_time_iter.pdf for the timing and graph of time per operation against number of iterations. These timings were measured with --yield disabled.


Questions 2.2
5 threads and 20 iterations seem to fairly consistently cause errors (including segfaults) with the various options of --yield.
The sync mechanisms were attempted but race conditions still appear to be present.


================================


Questions 3-1
1. The mutex must be held when pthread_cond_wait is called because pthread_cond_wait simultaneously unlocks the mutex it holds and begins waiting for the condition to be signaled. The thread waits until the condition is signaled as complete and the mutex is available.

2. The mutex must be released when the waiting thread is blocked because otherwise the thread that is waiting is also the thread that has the mutex lock and therefore is the only thread that can make the condition true for the thread to wake up.

3. The mutex must be reacquired when the calling thread resumes because otherwise, during the period between resuming and executing whatever needed to be done after waiting for the condition, another thread may have obtained the mutex and modified the condition.

4. The mutex must be released inside of pthread_cond_wait because if the mutex is released before calling pthread_cond_wait, antoher thread can acquire the mutex and change the condition to true and call pthread_cond_broadcast() or pthread_cond_signal() before pthread_cond_wait is called, then the waiting thread will not wake up.

5. This cannot be done in a user-mode implementation of pthread_cond_wait because pthread_cond_wait reaquires the mutex before the thread wakes up. In a user-mode implementation, this is not possible. If we try to reaquire the mutex after pthread_cond_wait returns, another thread may have obtained the mutex first.
